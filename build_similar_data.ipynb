{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/mnt/ebs/code/hallucination-mitigation/data/apps/train_deepseekcoder_mean_pooling.npy'\n",
    "test_file = '/mnt/ebs/code/hallucination-mitigation/data/apps/test_deepseekcoder_mean_pooling.npy'\n",
    "\n",
    "train_reps = np.load(train_file)\n",
    "test_reps = np.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairwise cosine similarities\n",
    "cosine_similarities = np.dot(test_reps, train_reps.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by the norms of the vectors\n",
    "test_norms = np.linalg.norm(test_reps, axis=1)\n",
    "train_norms = np.linalg.norm(train_reps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "(5000,)\n",
      "(5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(test_norms.shape)\n",
    "print(train_norms.shape)\n",
    "print(cosine_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cosine_similarities = cosine_similarities / np.outer(test_norms, train_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.979 , 0.972 , 0.9595, ..., 0.965 , 0.9795, 0.959 ], dtype=float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_cosine_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_cosine_similarities[0][1255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_cosine_similarities[0][4726]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 1 index for each test sample\n",
    "top_1_indices = np.argsort(updated_cosine_similarities, axis=1)[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4726])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 2 indices for each test sample\n",
    "top_2_indices = np.argsort(updated_cosine_similarities, axis=1)[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1255 4726]\n",
      "1255\n",
      "4726\n"
     ]
    }
   ],
   "source": [
    "print(top_2_indices[0])\n",
    "print(top_2_indices[0][0])\n",
    "print(top_2_indices[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, data_type):\n",
    "    data_path_1 = f\"/mnt/ebs/code/hallucination-mitigation/data/{dataset}/{data_type}_samples_1.jsonl\"\n",
    "    data_path_2 = f\"/mnt/ebs/code/hallucination-mitigation/data/{dataset}/{data_type}_samples_2.jsonl\"\n",
    "    df_1 = pd.read_json(data_path_1, lines=True)\n",
    "    df_2 = pd.read_json(data_path_2, lines=True)\n",
    "    # combine the two dataframes\n",
    "    df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(\"apps\", \"train\")\n",
    "test_df = load_data(\"apps\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dsc_context(problem):\n",
    "    prompt = \"\\nQUESTION:\\n\"\n",
    "    prompt += problem[\"question\"]\n",
    "    fn_name = problem[\"fn_name\"]\n",
    "\n",
    "    if not fn_name:\n",
    "        call_format = \"\\nPlease write your code using Standard Input, i.e. input() and print().\"\n",
    "        prompt += call_format\n",
    "    else:\n",
    "        call_format = \"\\Please write your code using Call-Based format.\"\n",
    "        prompt += call_format\n",
    "\n",
    "    system_prompt = \"You are an expert code developer with years of experience.\"\n",
    "    user_prompt = f'''As an expert code developer with years of experience, please provide the python code based on the question. Ensure the code is enclosed within triple backticks (```) to mark the start and end of the code block.\\n{prompt}'''\n",
    "\n",
    "    prompt = f'''You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "{user_prompt}\n",
    "### Response:\n",
    "'''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dsc_context_with_few_shot(test_problem, train_problems):\n",
    "    prompt = \"\\nQUESTION:\\n\"\n",
    "    prompt += test_problem[\"question\"]\n",
    "    fn_name = test_problem[\"fn_name\"]\n",
    "\n",
    "    if not fn_name:\n",
    "        call_format = \"\\nPlease write your code using Standard Input, i.e. input() and print().\"\n",
    "        prompt += call_format\n",
    "    else:\n",
    "        call_format = \"\\Please write your code using Call-Based format.\"\n",
    "        prompt += call_format\n",
    "\n",
    "    # add few-shot examples\n",
    "    example_prompt = \"\\n\\nEXAMPLES:\\n\"\n",
    "    for i in range(len(train_problems)):\n",
    "        train_problem = train_problems.iloc[i]\n",
    "        # print(train_problem.keys())\n",
    "        example_prompt += f\"\\nExample {i+1}:\\n\"\n",
    "        example_prompt += train_problem[\"question\"]\n",
    "        example_prompt += \"\\n\\nAnswer:\\n\"\n",
    "        # print(type(train_problem[\"solutions\"]))\n",
    "        example_prompt += train_problem[\"solutions\"][0]\n",
    "\n",
    "    system_prompt = \"You are an expert code developer with years of experience. You have been provided with a few examples to help you answer the question.\"\n",
    "    user_prompt = f'''As an expert code developer with years of experience, please provide the python code based on the question. You may consult the following example coding questions and their answers to provide the code. Ensure the code is enclosed within triple backticks (```) to mark the start and end of the code block.\\n{example_prompt}\\n{prompt}'''\n",
    "\n",
    "    prompt = f'''You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\n",
    "### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "{user_prompt}\n",
    "### Response:\n",
    "'''\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similar_data(num_shots, eval_indices, alpha):\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for i in tqdm(eval_indices):\n",
    "        test_problem = test_df.iloc[i]\n",
    "        top_n_indices = np.argsort(updated_cosine_similarities, axis=1)[:, -num_shots:]\n",
    "        train_indices = top_n_indices[i]\n",
    "        # get a df with the desired rows\n",
    "        train_problems = train_df.iloc[train_indices]\n",
    "\n",
    "        line_with_context = {\n",
    "            \"input_index\": test_problem[\"task_id\"],\n",
    "            \"assigned_model\": \"deepseek-ai/deepseek-coder-6.7b-base\",\n",
    "            \"assigned_process\": 0,\n",
    "            \"filter_p\": 1,\n",
    "            \"context_string\": build_dsc_context_with_few_shot(test_problem, train_problems),\n",
    "            \"assigned_weight\": 1+alpha\n",
    "            # Note: I do not give gold answers here because I do not use it for evaluation.\n",
    "            # I could also always access it using task_id.\n",
    "        }\n",
    "\n",
    "        line_without_context = {\n",
    "            \"input_index\": test_problem[\"task_id\"],\n",
    "            \"assigned_model\": \"deepseek-ai/deepseek-coder-6.7b-base\",\n",
    "            \"assigned_process\": 1,\n",
    "            \"filter_p\": 1,\n",
    "            \"context_string\": build_dsc_context(test_problem),\n",
    "            \"assigned_weight\": 0-alpha\n",
    "        }\n",
    "\n",
    "        output.append(line_with_context)\n",
    "        output.append(line_without_context)\n",
    "\n",
    "    # turn to pandas dataframe\n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_df.to_json(f\"/mnt/ebs/code/forked-context-aware-decoding/eval/apps/similar_{1+alpha}_-{alpha}_{num_shots}shots_eval300.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.08s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:26<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:26<00:00,  1.09s/it]\n",
      "100%|██████████| 300/300 [05:25<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "num_shots = 2\n",
    "eval_indices = [912, 204, 2253, 2006, 1828, 1143, 839, 4467, 712, 4837, 3456, 260, 244, 767, 1791, 1905, 4139, 4931, 217, 4597, 1628, 4464, 3436, 1805, 3679, 4827, 2278, 53, 1307, 3462, 2787, 2276, 1273, 1763, 2757, 837, 759, 3112, 792, 2940, 2817, 4945, 2166, 355, 3763, 4392, 1022, 3100, 645, 4522, 2401, 2962, 4729, 1575, 569, 375, 1866, 2370, 653, 1907, 827, 3113, 2277, 3714, 2988, 1332, 3032, 2910, 1716, 2187, 584, 4990, 1401, 4375, 2005, 1338, 3786, 3108, 2211, 4562, 1799, 2656, 458, 1876, 262, 2584, 3286, 2193, 542, 1728, 4646, 2577, 1741, 4089, 3241, 3758, 1170, 2169, 2020, 4598, 4415, 2152, 4788, 3509, 4780, 3271, 2965, 1796, 1133, 4174, 4042, 744, 385, 898, 1252, 1310, 3458, 4885, 520, 3152, 3126, 4881, 3834, 4334, 2059, 4532, 94, 938, 4398, 2185, 2786, 913, 2404, 3561, 1295, 3716, 26, 2157, 4100, 1463, 4158, 871, 2444, 4988, 1629, 3063, 1323, 4418, 4344, 4, 4906, 2655, 4002, 159, 916, 2973, 2519, 1961, 474, 1973, 4647, 701, 3981, 566, 4363, 1030, 1051, 3893, 4503, 1352, 2171, 4322, 4969, 3466, 1735, 4417, 1647, 2553, 3268, 3059, 3588, 4239, 3698, 991, 2030, 1840, 524, 2769, 172, 4819, 4537, 1885, 4820, 1804, 58, 581, 482, 1875, 552, 257, 2706, 580, 4211, 1949, 2281, 3976, 1755, 1083, 4677, 4720, 3872, 1990, 3874, 3334, 1559, 772, 794, 3531, 2902, 3469, 3367, 3825, 443, 806, 496, 3298, 2779, 895, 2036, 1569, 1558, 4393, 3675, 1148, 1503, 3789, 2046, 617, 3630, 4508, 802, 414, 4428, 120, 764, 1936, 1362, 3329, 3978, 3943, 1751, 3285, 480, 1348, 3104, 17, 3198, 2172, 3727, 2336, 3465, 4552, 3986, 1268, 1555, 2430, 1783, 479, 4744, 4441, 499, 2569, 468, 410, 4785, 3905, 4119, 4350, 1289, 465, 4160, 656, 1522, 561, 4874, 556, 1926, 3307, 982, 4666, 2016, 4742, 4870, 325, 671, 3434, 4781, 4630, 4282, 2591]\n",
    "for alpha in [0, 0.5, 1, 2, 4]:\n",
    "    for num_shots in [1, 2]:\n",
    "        build_similar_data(num_shots, eval_indices, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosa-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
